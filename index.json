[{"content":"The Monty Hall problem is an interesting puzzle in probability which has a counter-intuitive answer. In fact, it is documented that many people got the answer wrong and many people continue to get the answer wrong.\nThe problem The problem goes somewhat like this:\nImagine a game show. There are 3 doors and behind one door there is a car and behind the other 2, goats. The player picks a door hoping to win a car that\u0026rsquo;s behind the door. The game show host opens one of the other doors to reveal a goat (always). Does the player need to switch the choice of his door or stick with his first guess?\nWhen I first saw the problem, I went headlong into solving it and got the answer wrong.\nThe common mistake The usual first solution that is offered is to not switch the chosen door after one of the doors is opened. After this step, most people think there is equal probability of the car being in either of the remaining closed doors. Lot of people got the answer wrong when it was first posed and continue to get it wrong. Wikipedia has documented many reasons why people make this choice of not wanting to switch which is an interesting read.\nThe actual solution (Spoiler alert!) The correct answer is to switch the choice of the door after the game host has revealed a goat by opening one of the two closed doors.\nAn intuitive explanation There is a purely probability based solution described in many places on the internet, which is somewhat of a hard read. Yes, I looked up the answer, but it was kind of unsatisfying, mainly because it was hard to intuitively grasp the answer based on probability. While thinking over this on and off, I chanced to come up with an alternative solution which is also presented elsewhere on the internet.\nTo begin with, there is one car and two goats behind 3 closed doors. So this essentially means there is a higher probability of picking a door with a goat behind it. Let\u0026rsquo;s recall from the definition of probability, in our case:\nThe probability of picking a goat is:\nNumber of goats / total number of all things (goats and cars) = 2 / 3\nSo, statiscally, there is a higher chance of picking a door with a goat behind it in the beginning. Therefore, it makes sense to switch the door after the game show host has opened to reveal a goat behind one of the closed doors.\nIn fact, even if the problem were to be modified as I explain below, the correct answer is to always switch the door.\nLet\u0026rsquo;s say we have 100 closed doors, with 99 goats and 1 car behind them. Now you can immediately see there is more chance (99/100 to be exact) of somebody randomly picking a door with a goat behind it. Now if the game show host opens 98 of the 99 closed doors to reveal 98 goats behind them, leaving only one door unopened, then doesn\u0026rsquo;t it make sense to switch the door that you initially picked?\nSimulation Let\u0026rsquo;s create a simulation to drive home this intuition and to prove that this is indeed the correct answer.\nFirst, let\u0026rsquo;s write a function to create a game. The game consists of a number of doors with 1 car and the rest of them goats. The function returns an array whose elements represent doors and what\u0026rsquo;s behind them, i.e., 1 for car and 0 for goats.\nNow let\u0026rsquo;s run it to see that indeed the car is behind a different door in each game instance (statistically speaking).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import numpy as np def create_game(num_doors=3): \u0026#34;\u0026#34;\u0026#34;Create and return number of doors with one car (represented by 1) in a random position and the rest with goats (represented by 0).\u0026#34;\u0026#34;\u0026#34; # Initialize random number generator rng = np.random.default_rng(np.random.randint(123456789)) doors = np.zeros((num_doors,), dtype=int) # 1 = car, 0 = goat. Initialize car behind one of the doors picked at random. doors[rng.integers(num_doors)] = 1 return doors # Create games with 3 doors print(\u0026#34;Games with 3 doors:\u0026#34;) for i in range(5): new_game_door = create_game(num_doors=3) print(new_game_door) # Create games with 10 doors print(\u0026#34;\\nGames with 10 doors:\u0026#34;) for i in range(5): new_game_door = create_game(num_doors=10) print(new_game_door) Games with 3 doors: [0 1 0] [1 0 0] [0 1 0] [0 0 1] [0 0 1] Games with 10 doors: [0 0 0 0 0 0 1 0 0 0] [0 0 0 0 0 0 0 1 0 0] [0 0 0 0 0 0 1 0 0 0] [0 0 0 0 0 0 0 0 0 1] [0 0 0 0 0 0 0 0 0 1] We can see that the car (1) is set in the arrays above at random positions within each array.\nLet\u0026rsquo;s create a function to select a door in an instance of the game. This function will return the player\u0026rsquo;s choice of the door by sampling a random number from the number of doors.\n1 2 3 4 5 6 7 8 import numpy as np def select_door(game): \u0026#34;\u0026#34;\u0026#34;Select a door from the game (player\u0026#39;s choice at the beginning). Return the index of the selected door.\u0026#34;\u0026#34;\u0026#34; selected_door = np.random.randint(len(game)) return selected_door We then simulate the part where the game show host reveals all the doors that have goats behind them except for one door and the player ends up with two doors to figure out what to do next.\nWe can do this by creating a new array that represents the doors that includes the value of the index (door) that was chosen by the player and the value of the remaining item in the array after revealing all the goats.\nIf the player had selected a door with a car behind it (value 1), then the remaining doors are all goats (0s). The new array will contain elements [1, 0].\nIf the player had selected a door with a goat behind it (value 0), the remaining doors have one car (value 1) and the rest goats (0s). So the new array will have the elements [0, 1] because the game show host has to reveal only the doors with goats behind them which leaves one unopened door with a car behind it.\n1 2 3 4 5 6 7 8 9 10 11 12 13 import numpy as np def reveal_goats(selected_door): \u0026#34;\u0026#34;\u0026#34;Function that returns an array representing the two doors after the game show host has revealed all the goats. The first element in the array represents the door that the player had picked in the beginning. The second element in the array represents the remaining door after rest of the doors were opened to reveal goats.\u0026#34;\u0026#34;\u0026#34; if selected_door == 1: return np.array([1, 0]) else: return np.array([0, 1]) Now, the following two cases are possible:\nPlayer switches the door When the player switches the door, the result is given by the second element of the two element array.\nPlayer does not switch In this case, the result is given by the first element of the two element array.\n1 2 3 4 5 def result(remaining_doors, switch=True): if switch == True: return remaining_doors[1] else: return remaining_doors[0] Results Now that we have all the pieces, we can put them together and run the simulation to study the overall winnings when the player switches and when they do not.\nThe entire code is shown below and is also available in my github repo.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 import numpy as np import matplotlib.pyplot as plt def create_game(num_doors=3): \u0026#34;\u0026#34;\u0026#34;Create and return number of doors with one car (represented by 1) in a random position and the rest with goats (represented by 0).\u0026#34;\u0026#34;\u0026#34; # Initialize random number generator rng = np.random.default_rng(np.random.randint(123456789)) doors = np.zeros((num_doors,), dtype=int) # 1 = car, 0 = goat. Initialize car behind one of the doors picked at random. doors[rng.integers(num_doors)] = 1 return doors def select_door(game): \u0026#34;\u0026#34;\u0026#34;Select a door from the game (player\u0026#39;s choice at the beginning). Return the index of the selected door.\u0026#34;\u0026#34;\u0026#34; selected_door = np.random.randint(len(game)) return selected_door def reveal_goats(selected_door): \u0026#34;\u0026#34;\u0026#34;Function that returns an array representing the two doors after the game show host has revealed all the goats. The first element in the array represents the door that the player had picked in the beginning. The second element in the array represents the remaining door after rest of the doors were opened to reveal goats.\u0026#34;\u0026#34;\u0026#34; if selected_door == 1: return np.array([1, 0]) else: return np.array([0, 1]) def result(doors, switch=True): \u0026#34;\u0026#34;\u0026#34;Function that returns the eventual choice of the player. If the player switches, return the second element in the array, else return the first.\u0026#34;\u0026#34;\u0026#34; if switch is True: return doors[1] else: return doors[0] # Play 100 times the game of 3 doors. accum_result_no_switch = [] accum_result_switch = [] res_no_switch = 0 res_switch = 0 for i in range(100): new_game = create_game(num_doors=3) selected_door = select_door(new_game) final_doors = reveal_goats(selected_door) res_no_switch += result(final_doors, switch=False) res_switch += result(final_doors, switch=True) accum_result_no_switch.append(res_no_switch) accum_result_switch.append(res_switch) plt.plot(np.arange(len(accum_result_no_switch)), accum_result_no_switch, label=\u0026#39;Accumulated Wins (No Switch)\u0026#39;) plt.plot(np.arange(len(accum_result_switch)), accum_result_switch, label=\u0026#39;Accumulated Wins (Switch)\u0026#39;) plt.xlabel(\u0026#34;Number of games played\u0026#34;) plt.ylabel(\u0026#34;Wins\u0026#34;) plt.title(\u0026#34;Number of Doors = 3\u0026#34;) plt.legend() plt.savefig(\u0026#34;100games3doors.png\u0026#34;) # The next return statement was needed for org-mode for creating this blog return \u0026#34;100games3doors.png\u0026#34; As we can see in the plot, the overall winnings is higher in the long run if the player chooses to switch the door. This is of course a statistical result, which means that there is about 1/3 probability of the player winning even if he chooses not to switch. The plot is very similar to this wikipedia version. Due to the probabilistic nature of the game/simulation, the plots will look slightly different each time the simulation is run.\nAnd now! Monty Hall problem with 100 doors ;) If you run the simulation for a 100 door game (1 car, 99 goats), it\u0026rsquo;s quite evident from the plot below that it makes perfect sense to switch! The chances of winning in this case if you don\u0026rsquo;t switch is miniscule. And now! Monty Hall problem with a million doors Ok, now I know I am pushing it. Until later, bye and hope this was interesting and fun to read.\n","permalink":"http://shalbhav.github.io/posts/monty-hall-sim/","summary":"The Monty Hall problem is an interesting puzzle in probability which has a counter-intuitive answer. In fact, it is documented that many people got the answer wrong and many people continue to get the answer wrong.\nThe problem The problem goes somewhat like this:\nImagine a game show. There are 3 doors and behind one door there is a car and behind the other 2, goats. The player picks a door hoping to win a car that\u0026rsquo;s behind the door.","title":"The Monty Hall Problem: Intuition and Simulation"},{"content":"Life needs energy to survive. Energy is needed to perform basic life functions like movement, hunting and gathering, protecting from prey and natural elements. It is converted from one form to another. Eventually, the energy within the living organism is expelled from the organism while useful work is performed.\nWhy and how energy for life? Work is done when there is change in energy. In other words, energy has to go somewhere, so work gets done.\nWhile humans and other animals do physical activity like walking or running, the energy in their cells is eventually converted to carbon dioxide and heat during metabolism, both of which are expelled from the organism. The environment is in a state to receive these byproducts. In this way, the overall energy difference between the organism and the environment slightly decreases.\nWe then ingest energy from outside the environment and replenish our energy supply, thus raising the energy available to us. This causes the overall energy difference between the organism and the environment to increase. This assumes that there is an infinite source of energy, but that is not the case. Think of it this way, if at the beginning, Life had 10J of energy and Environment had 100J of energy available, the difference in energy between the two is 90J. If later, Life consumed 20J from Environment, the score is now: Life=30J, Environment=80J. The difference however is now 50J; it has reduced.\nThe form of energy due to work done is eventually mostly lost as heat (Ref. 1), and we will approach a state where energy will be evenly distributed. In our example, if Life and Environment approach equal energy, there is no way for one to borrow energy from the other. If we extrapolate this to everything in the universe, it will eventually lead to the Heat death of the Universe - a thermodynamic equilibrium - insert \u0026lt; a vague, sinking feeling here \u0026gt;.\nThe energy difference between the life forms and the environment can decrease over time \u0026ndash; food sources can vanish and water bodies can dry up. Remembering that energy is the ability to do work, life forms can no longer survive in that immediate environment and organisms will need to forage for food sources further away.\nWhat is Entropy? Entropy is the relative distribution of energy in a closed system. The earth can be thought of a closed system where living organisms ingest food from the environment. One can also think of life as an open system which interacts with the environment and continously draw energy from it \u0026ndash; also known as negentropy.\nThus energy is redistributed among different living things as they go through the life and death process. In other words, living beings need to be in a state of higher energy with respect to their environment, thus lowering their entropy. And remembering once again, work is done only when there is a energy difference between the initial state of the system and the final state.\nIn the end Life is a low entropy system. When death comes about, the organism is no longer able to feed itself and raise it\u0026rsquo;s energy (or lower it\u0026rsquo;s entropy) and it\u0026rsquo;s energy is assimilated or diffused into the environment, thus the overall entropy of the system increases. This is in accordance with the second law of thermodynamics which states that the entropy of the universe is always increasing.\nSo in life and death, the second law of thermodynamics makes it\u0026rsquo;s point.\nReferences Heat death from Wikipedia What is Life? by Erwin Schrodinger While there were many sources where I read and heard about this topic, I was pleasantly surprised to see this being expounded by none other than Erwin Schrodinger in his book What is Life? - the excerpt of which is below. ","permalink":"http://shalbhav.github.io/posts/entropy-and-life/","summary":"Life needs energy to survive. Energy is needed to perform basic life functions like movement, hunting and gathering, protecting from prey and natural elements. It is converted from one form to another. Eventually, the energy within the living organism is expelled from the organism while useful work is performed.\nWhy and how energy for life? Work is done when there is change in energy. In other words, energy has to go somewhere, so work gets done.","title":"Entropy and Life"},{"content":"Emacs org-mode is an emacs major mode used for note keeping, project planning and for creating documents and articles. It has many export options from markdown to pdf to latex. While all this is great for document generation, it can also be used for creating presentations with org-reveal. Still, this needs a browser and sometimes you really have more text than a few bullet points and want a simpler option to just focus on the text under each heading by narrowing to each heading as you present.\nOrg-mode has the commands to move to next and previous headings and this combined with narrowing and widening options can really help one to focus on the text that is being presented at the moment.\nThe following elisp functions define how to focus the org text on each heading. Place this in the init.el emacs configuration file and bind it to the keys of your choice.\nThe functions use the org-mode builtin functions org-forward-heading-same-level and org-backward-heading-same-level to move to headings. Since these commands are themselves interactive commands we need to call them using the call-interactively function. widen is needed to make headings momentarily visible so that we can jump to the next/previous headings. Call org-narrow-to-subtree to focus only on the heading to be presented.\n(defun narrow-to-next-heading () \u0026#34;Narrow to next heading\u0026#34; (interactive) (widen) (call-interactively \u0026#39;org-forward-heading-same-level nil) (org-narrow-to-subtree)) (with-eval-after-load \u0026#39;org (add-hook \u0026#39;org-mode-hook (lambda () (local-set-key (kbd \u0026#34;C-c f\u0026#34;) \u0026#39;narrow-to-next-heading)))) (defun narrow-to-prev-heading () \u0026#34;Narrow to prev heading\u0026#34; (interactive) (widen) (call-interactively \u0026#39;org-backward-heading-same-level nil) (org-narrow-to-subtree)) (with-eval-after-load \u0026#39;org (add-hook \u0026#39;org-mode-hook (lambda () (local-set-key (kbd \u0026#34;C-c b\u0026#34;) \u0026#39;narrow-to-prev-heading)))) To highlight the current subtree or heading, you can use (org-narrow-to-subtree) / (C-x n s) with cursor inside the block. To move to next/previous subtrees/headings you can use the keybindings defined for those functions. In my case, I\u0026rsquo;ve bound them to C-c f and C-c b to move forward and backward in the subtrees respectively.\nTo get back to the entire document run C-x n w to widen. The gif shows how this works in action.\nTODO\nIt would be nice to increase/decrease font size of the highlighted section based on the current window size.\nShortly after I did this, I learnt of a emacs package called org-present which might do all this and more. I plan on trying it out sometime in the future. If nothing, this helped me to brush up on some emacs lisp.\n","permalink":"http://shalbhav.github.io/posts/org-mode-present-focus/","summary":"Emacs org-mode is an emacs major mode used for note keeping, project planning and for creating documents and articles. It has many export options from markdown to pdf to latex. While all this is great for document generation, it can also be used for creating presentations with org-reveal. Still, this needs a browser and sometimes you really have more text than a few bullet points and want a simpler option to just focus on the text under each heading by narrowing to each heading as you present.","title":"Presenting with Emacs Org-mode"},{"content":"While I was refreshing some concepts in statistics, I came up with a problem I thought I could ask ChatGPT (talk about attention deficit). I am still using the cheap ChatGPT3.5, so what I am about to show may not work on the expensive ChatGPT4.0. Here\u0026rsquo;s an example on how it can mislead while solving math problems and one should already have an inkling how to solve the problem (or atleast have an estimate of the answer) to do some prompt engineering to finally get to a solution.\nHere\u0026rsquo;s the conversation (edited out quite a bit of fluff). My prompts are in italics.\nThe conversation Me: What is the probability of 3 samples from a normal distribution to be one standard deviation more than the mean?\nChatGPT: In a standard normal distribution (with a mean of 0 and a standard deviation of 1), the probability that a random variable falls within one standard deviation of the mean is approximately 68.27%. This is a well-known property of the normal distribution. The probability that the first sample is one standard deviation more than the mean is approximately 0.6827 (68.27%).\nSince each sample is independent, the probability that the second sample is also one standard deviation more than the mean is also 0.6827 (68.27%).\nSimilarly, the probability that the third sample is one standard deviation more than the mean is 0.6827 (68.27%).\nTo find the combined probability that all three events occur, you simply multiply these probabilities together because they are independent:\n0.6827 * 0.6827 * 0.6827 ≈ 0.3194\nMe (to myself): That seems wrong, so let\u0026rsquo;s prompt again\u0026hellip;\nMe (to ChatGPT): What is the probability of a sample from a normal distribution to be more than one standard deviation away from the mean?\nChatGPT: In a standard normal distribution (with a mean of 0 and a standard deviation of 1), the probability that a random variable falls more than one standard deviation away from the mean (in either direction) is approximately 31.73%. Using standard normal tables or a calculator, you can find the probabilities associated with Z \u0026lt; -1 and Z \u0026gt; 1, which are approximately 0.1587 and 0.1587, respectively.\nNow, add these probabilities:\n0.1587 + 0.1587 ≈ 0.3174\nSo, the probability that a sample from your normal distribution is more than one standard deviation away from the mean is approximately 31.74%.\nMe: Does this change your answer to the previous question?\nYes, the probability that a sample from a normal distribution is more than one standard deviation away from the mean is approximately 31.74%, as calculated in the previous response. This probability applies to each individual sample.\nIn the previous question, you asked about the probability that three samples from the same distribution are each one standard deviation more than the mean.\nTherefore, the probability that all three samples are each one standard deviation more than the mean remains the same:\n0.3174 * 0.3174 * 0.3174 ≈ 0.0320 or approximately 3.20%.\nConcluding thoughts The astute reader will recognize that it\u0026rsquo;s still wrong, since I am asking for the probability of the sample being one std more than the mean, whereas it\u0026rsquo;s still calculating the answer for one std away from the mean (i.e., it\u0026rsquo;s calculating the probability that the points are on both sides of the distribution instead of calculating for the points being on the upper side of the distribution).\nThe correct answer is (0.1587)^3.\nAnyway, thought I\u0026rsquo;d share this non-wordly stuff. Can\u0026rsquo;t fault it, it\u0026rsquo;s an Large Language Model, not an Large Math Model\u0026hellip; yet (weird, evil, nervous laugter, umbwaahhaaha).\n","permalink":"http://shalbhav.github.io/posts/chatgpt-llm-not-lmm/","summary":"While I was refreshing some concepts in statistics, I came up with a problem I thought I could ask ChatGPT (talk about attention deficit). I am still using the cheap ChatGPT3.5, so what I am about to show may not work on the expensive ChatGPT4.0. Here\u0026rsquo;s an example on how it can mislead while solving math problems and one should already have an inkling how to solve the problem (or atleast have an estimate of the answer) to do some prompt engineering to finally get to a solution.","title":"ChatGPT is an LLM and not an LMM (yet)"},{"content":"Mosquito bats work great for killing mosquitoes and flies. Until they don\u0026rsquo;t work! They hardly last one mosquito generation. It turns out that the rechargeable battery in them go bad fairly quickly. So as a small hack, I took out the cheap battery they come with and replaced it with a decent rechargeable LiIon 3.7v 2200mah one. You have to modify the battery housing a little bit by carving out some extra bits of plastic to make the 18650 type battery to fit. It seems a bit more solid now. Now the mosquitoes won\u0026rsquo;t know what zapped their asses.\nMyaahhaaha!\n","permalink":"http://shalbhav.github.io/posts/extend-life-mosquito-bat/","summary":"Mosquito bats work great for killing mosquitoes and flies. Until they don\u0026rsquo;t work! They hardly last one mosquito generation. It turns out that the rechargeable battery in them go bad fairly quickly. So as a small hack, I took out the cheap battery they come with and replaced it with a decent rechargeable LiIon 3.7v 2200mah one. You have to modify the battery housing a little bit by carving out some extra bits of plastic to make the 18650 type battery to fit.","title":"Extending life of a Mosquito Bat"},{"content":"The Marangoni effect is a phenomenon in fluid dynamics where a change in surface tension along the interface of two fluids causes the movement of liquid (Ref. 1). This effect can be demonstrated using a toy boat with soap or camphor mounted at the center/rear of the boat touching the water. The dissolved soap or camphor causes a reduction of surface tension of water at the back of the boat compared to the surface tension of water at the front. This effectively acts as a little motor that can propel the toy boat.\nThe Toy Boat To test this out I made a tiny boat (2cm x 1cm) out of lightweight wood with a notch at the rear of the boat to mount a camphor tablet. The position of the notch allows the camphor to be placed a little closer to the center of the boat whilst still allowing the camphor to touch the water.\nFigure 1: Little Lightweight Toy Boat\nWhether one is 50 or 15, you cannot help but feel like a 5 year old when you see a boat autonomously circling around in a bowl of water powered by a humble camphor tablet (or a small piece of soap even).\nFigure 2: Propelled by a Camphor motor\nForces at play In general the boat moves due to a surface tension gradient caused by the camphor concentration. This causes the surface tension to be more ahead of the boat and creates a net postive force on the front of the boat. The following picture captures the different forces.\nFigure 3: Balance of forces\nIn the case of a body that wets completely, the angles \\( \\alpha \\) and \\( \\beta \\) would be \\( 90\\degree \\), and the horizontal forces on the boat will simply be equal to the respective surface tensions, \\( \\sigma_0 \\) and \\( \\sigma_c \\).\nSome intuition The surface tension gradient is changing over time due to the diffusion of camphor in water. The following picture shows the angles formed by the surface of the water with the object. The angle of contact due to the surface tensions will be different depending on the concentration of camphor, which is higher at the rear of the boat, resulting in lowering of surface tension. As the concentration gradient moves towards the front of the boat, the region of higher surface tension becomes a region of lower surface tension and causes the object to move over time. It helps me to visualize this as a traveling wave.\nFigure 4: Concentration gradient causing movement\nAt the deeper end There would be some amount of fluidic friction due to viscosity which creates a drag on the overall force, which is not captured above. At very small scales all of these forces would matter and the behavior of objects due to these tiny forces cause interesting effects.\nThe concentration gradient is usually modeled using reaction-diffusion equations (Ref. 2), so called because, taking our current example, there is a reaction that causes the camphor molecules to mix with water, and a diffusion phenomena that causes these molecules to spread in time.\nActual measurements done in a more calibrated environment shows that difference in surface tensions to be in the order of 1.1 mN/m and the speed of the boat to be around 66 mm/s. In our rough home experiment, we can atleast see that the boat moves a few cms/second.\nThis turned out to be a fun and interesting experiment and all errors in the analysis is of course my own.\nReferences Wikipedia on Marangoni Effect Quantitative Estimation of the Parameters for Self-Motion Driven by Difference in Surface Tension ","permalink":"http://shalbhav.github.io/posts/marangoni-toy-boat/","summary":"The Marangoni effect is a phenomenon in fluid dynamics where a change in surface tension along the interface of two fluids causes the movement of liquid (Ref. 1). This effect can be demonstrated using a toy boat with soap or camphor mounted at the center/rear of the boat touching the water. The dissolved soap or camphor causes a reduction of surface tension of water at the back of the boat compared to the surface tension of water at the front.","title":"Marangoni's Toy Boat"},{"content":"Apparently a container does not even need an OS whereas VMs do. Container != VM, but I never stopped to think about this before. Processes within the container can directly talk to the kernel on which the container is running. Most containers are built with a base OS image. Turns out you can create an image without a base OS by pulling the scratch image, that contains nothing.\nLet\u0026rsquo;s create a small c program that is statically compiled.\n$ cat small.c #include \u0026lt;unistd.h\u0026gt; int main() { write(1, \u0026#34;Hi!\u0026#34;, 3); } $ gcc -o small small.c --static Just 852kB!\n$ ls -l small --block-size=k -rwxrwxr-x 1 username username 852K Aug 5 15:34 small This executable has everything it needs to run and does not need an OS. There is nothing specific to Ubuntu, for example. It just prints \u0026ldquo;Hi!\u0026rdquo;.\n$ ./small Hi! Let\u0026rsquo;s build a Docker image with this executable inside it.\n# Run this in the same directory where the executable \u0026#39;small\u0026#39; is. $ cat Dockerfile FROM scratch ADD small /small CMD [\u0026#34;/small\u0026#34;] $ docker build -t small-image -f Dockerfile . $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE small-image latest 46ad701a4cce About an hour ago 872kB It\u0026rsquo;s about 872kB which is almost the same size as the executable small. This should convince you that the scratch image has nothing in it and we can verify this a functional image without a base OS, directly talking to the kernel.\n$ docker run small Hi! Hope you found this useful!\n","permalink":"http://shalbhav.github.io/posts/container-no-os/","summary":"Apparently a container does not even need an OS whereas VMs do. Container != VM, but I never stopped to think about this before. Processes within the container can directly talk to the kernel on which the container is running. Most containers are built with a base OS image. Turns out you can create an image without a base OS by pulling the scratch image, that contains nothing.\nLet\u0026rsquo;s create a small c program that is statically compiled.","title":"A container does not need an OS"},{"content":"Kubernetes Horizontal Pod Autoscaling provides a way to scale up number of Pod replicas in a deployment based on per-pod metric thresholds set by the operator. Most often the per-pod resource metrics (like CPU and memory) are used to scale the pods. The utilization value is computed over all the containers in each Pod.\nStarting with v1.20, Kubernetes started offering HPA scaling based on container resource metrics as an alpha feature (as of the time of this writing). This offers another way to scale the target resource based on container metrics. Pods can have multiple containers, so this allows the user to choose the metric utilization over a specific container of all the Pods.\nThere are implications due to this. Most notably, as mentioned in the official documentation example, if you have a web application and a logging sidecar, you can scale based on the resource utilization of the web application, ignoring the sidecar container. This could be extended to other applications with multiple sidecars. As I describe next, there are certain constraints under which this method of scaling should be used.\nLet\u0026rsquo;s say we have a Pod with two containers A and B, and we want to scale on resource utilization of container A.\nLet \\(U_{a}\\) and \\(U_{b}\\) be the resource utilizations of containers A and B. Let \\(Q_{a}\\) and \\(Q_{b}\\) be the metric request setting of the two containers defined in the Pod Spec.\nIn the per-pod resource metric based scaling, the HPA will scale based on the utilization using the following formula:\nPer-pod based usage:\n\\begin{equation} P_{u} = \\frac{U_{a} + U_{b}}{Q_{a} + Q_{b}} \\end{equation}\nIn the container resource metric based scaling, the HPA will scale based on the utilization of container A, using the following formula:\nContainer based usage:\n\\begin{equation} C_{u} = \\frac{U_{a}}{Q_{a}} \\end{equation}\nFor the same utilizations and request settings in both scenarios, the HPA will scale Pods earlier while using container resource metrics compared to per-pod resource metrics; only if the resource utilization of container B is less than container A, i.e.,\nif\n\\begin{equation} \\frac{U_{b}}{Q_{b}} \u0026lt; \\frac{U_{a}}{Q_{a}} \\end{equation}\nBy substituting \\( U_{b} \u0026lt; \\frac{U_{a}Q_{b}}{Q_{a}} \\) into \\( (1) \\) and simplifying, we get:\n\\begin{equation} \\frac{U_{a} + U_{b}}{Q_{a} + Q_{b}} \u0026lt; \\frac{U_{a}}{Q_{a}} \\end{equation}\nThe resource utilization of container A will meet the threshold earlier using container based usage and so HPA scales this earlier - only if the resource utilization of the other containers are less compared to the container (A) we are interested in. So make sure that the other container resource usages are well below the container over which you are scaling, otherwise scaling may be delayed affecting performance.\n","permalink":"http://shalbhav.github.io/posts/k8s-hpa-algebra/","summary":"Kubernetes Horizontal Pod Autoscaling provides a way to scale up number of Pod replicas in a deployment based on per-pod metric thresholds set by the operator. Most often the per-pod resource metrics (like CPU and memory) are used to scale the pods. The utilization value is computed over all the containers in each Pod.\nStarting with v1.20, Kubernetes started offering HPA scaling based on container resource metrics as an alpha feature (as of the time of this writing).","title":"Analyzing Kubernetes container based scaling"},{"content":"From my 5 year old silly self to your 5 year old selves. A ditty for the beginning of this year.\n2023\nRoam the world; land and sea.\nMind unshackled, free,\nFind life\u0026rsquo;s key.\n2023\nBe silly,\nGet tipsy,\nSport a goatee,\nOr wear a teepee.\n2023\nBe a busy bee,\nGo on a shopping spree,\nGrow a muscle or three.\n2023\nSpread happy,\nJoy and glee.\n2023\nGrow a giant tree!\n2023\nBetween a yep and nope,\nMaybe leave room for hope,\nThat maybe.. there\u0026rsquo;s a maybe.\nTee hee hee.\n","permalink":"http://shalbhav.github.io/posts/2023/","summary":"From my 5 year old silly self to your 5 year old selves. A ditty for the beginning of this year.\n2023\nRoam the world; land and sea.\nMind unshackled, free,\nFind life\u0026rsquo;s key.\n2023\nBe silly,\nGet tipsy,\nSport a goatee,\nOr wear a teepee.\n2023\nBe a busy bee,\nGo on a shopping spree,\nGrow a muscle or three.\n2023\nSpread happy,\nJoy and glee.\n2023\nGrow a giant tree!","title":"2023"},{"content":"I came across this probability problem and thought it was interesting.\nWhat is the expectation of distance (from the center) of a circular disc to uniformly distributed random points on the disc?\nIf you pick a random point (or throw a dart) on a circle, there are more chances of the point landing somewhere between the center and the perimeter of the circular disc. Intuitively, this is because there are more points on the disc, i.e., more area available, as we move away from the center and so there is a higher probability of the random point being away from the center. So, in expectation, it seems like the distance of random points on the circular disc must be greater than half the radius. Let\u0026rsquo;s see how to get the exact value.\nConsider this circle of radius R and a random point about a distance r from the center. Now consider a very small circular strip of thickness dx at r.\nSo the average distance of a random point from the center of a circle of radius R is given by \\( \\frac{2*R^2}{3} \\).\nFor a unit circle, the average or the expectation of the distance of random points on a circle is 2/3.\n","permalink":"http://shalbhav.github.io/posts/expected-distance-disc/","summary":"I came across this probability problem and thought it was interesting.\nWhat is the expectation of distance (from the center) of a circular disc to uniformly distributed random points on the disc?\nIf you pick a random point (or throw a dart) on a circle, there are more chances of the point landing somewhere between the center and the perimeter of the circular disc. Intuitively, this is because there are more points on the disc, i.","title":"Expectation of distances of random points on a circular disc"}]